{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Convolution Neural Networks.\n",
    "\n",
    "What are CNNs?\n",
    "Well, CNN are type of neural network where in we convolute input/hidden layer with the weights. \n",
    "Weights in typical neural network is one-to-one mapped to each input/intermediate neurons. But,\n",
    "in CNN, weights are shared between the patches of input/intermediate neurons.\n",
    "\n",
    "Why share weights and what imapct does it have on neural networks?\n",
    "Sharing weights solves two main problems </p> a) **translation invariance**; b) **Number of parameters to train in Networks**\n",
    "\n",
    "### a) translation invariance:\n",
    "\n",
    "Suppose you are training a neural network to detect a cat in a image. You really don't care where is the cat in the image, you want your model to be robust to detect cat irrespective of its position in the image, because a cat in right side of image is same as cat in left side of image.\n",
    "\n",
    "If we train a traditional neural netwwork we may need to train every parameter(weights associated with each input pixel, teaching same thing to different neuron to detect the cat in different parts of image. And it fails to generalize.\n",
    "\n",
    "This can be well handled by CNN were we share weights(patch/kernel) across the image. In CNN, parameters are shared acrossed space, similarly, in RNN it is shared accrossed time. Sharing weights help the cnn to detect already learnt object without retraining another set of parameters.\n",
    "\n",
    "![parts of CNN](./images/CNN_01.png)\n",
    "\n",
    "### b) Number of parameters to train in Networks\n",
    "\n",
    "Number of parameters to train in neural networks is very important, because number of parameters is directly proportional to the time taken by nueral network to train. SO, while build a deep model(NN with many hidden layers) It is utmost important to know how big is NN in terms of parameters to trainig.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality\n",
    "\n",
    "how can we calculate the number of neurons of each layer in our CNN?\n",
    "\n",
    "Given:\n",
    "\n",
    "our input layer has a width of W and a height of H\n",
    "our convolutional layer has a filter size F\n",
    "we have a stride of S\n",
    "a padding of P\n",
    "and the number of filters K,\n",
    "the following formula gives us the width of the next layer: W_out = [(Wâˆ’F+2P)/S] + 1.\n",
    "\n",
    "The output height would be H_out = [(H-F+2P)/S] + 1.\n",
    "\n",
    "And the output depth would be equal to the number of filters D_out = K.\n",
    "\n",
    "The output volume would be W_out * H_out * D_out.\n",
    "\n",
    "Knowing the dimensionality of each additional layer helps us understand how large our model is and how our decisions around filter size and stride affect the size of our network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdc",
   "language": "python",
   "name": "sdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
