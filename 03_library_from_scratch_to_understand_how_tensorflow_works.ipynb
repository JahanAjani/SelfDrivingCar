{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 3:\n",
    "writing your own library like tensorflow, A MiniFlow library.\n",
    "\n",
    "Before we start using tensorflow, keras or anyother library, it is best to learn how this library works conceptually. So, to do that we will write our own library - MiniFlow which will work similiarly like tensorFlow. Why this is important? Well, before using this abstractions Isn't it is good to learn how under the hood this library works? forward pass, backprop, derivatives or chain rule?\n",
    "\n",
    "An intellectually curious mind should know this nutss and bolts and that is why we will write our first Neural Network in from scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniFlow Architecture:\n",
    "\n",
    "Let's consider how to implement this graph structure in MiniFlow. We'll use a Python class to represent a generic node.\n",
    "\n",
    "We know that each node might receive input from multiple other nodes. We also know that each node creates a single output, which will likely be passed to other nodes. Let's add two lists: one to store references to the inbound nodes, and the other to store references to the outbound nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node(object):\n",
    "    def __init__(self, inbound_nodes = []):\n",
    "        #Nodes from which this Node recieves values\n",
    "        self.inbound_nodes = inbound_nodes\n",
    "        #Nodes to which this node pass values\n",
    "        self.outbound_nodes = []\n",
    "        #for each inbound_nodes add this Node as outbound Node.\n",
    "        for inbound_node in self.inbound_nodes:\n",
    "            inbound_node.outbound_nodes.append(self)\n",
    "        #A calculated final value of this Node\n",
    "        self.value = None\n",
    "        \n",
    "    def forward(self):\n",
    "        '''\n",
    "        Forward propagation.\n",
    "        \n",
    "        compute the output value based on 'inbound_nodes' and \n",
    "        store the result in self.value\n",
    "        '''\n",
    "        raise NotImplemented"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*While Node defines the basic set of properties that every node holds, only specialized subclasses of will end uo in final graph. So, lets build our first subclass which will calculate the value and hold value.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Input(Node):\n",
    "    def __init__(self):\n",
    "        #An Input Node has no inbound nodes,\n",
    "        #so no need to pass anything to the Node instantiator.\n",
    "        Node.__init__(self)\n",
    "    \n",
    "    def forward(self, value=None):\n",
    "        '''\n",
    "        since Input node is the node which doesn't have any inbound_nodes\n",
    "        this forward method will take value as input and set it self.value\n",
    "        while, other non input Node's forward method will read the value form each inbound_nodes.value\n",
    "        calculate the resultant and store it in self.value.\n",
    "        '''\n",
    "        #overwite the value if one passed in.\n",
    "        if value is not None:\n",
    "            self.value = value\n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Okay, so far we wrote Input Node which doesn't take any input from other nodes. But itself holds the input to neural network and this value can be set directly by setting Node.vlaue or by passing value to forward method.*\n",
    "\n",
    "*As we know, in nueral networks there are nodes which takes value from such input nodes or hidden nodes perform actual calculation and save it along with passing it to rest of network.*\n",
    "\n",
    "*Okay, Lets implement Add Node which will does exactly this.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Add(Node):\n",
    "    def __init__(self, inbound_nodes = []):\n",
    "        Node.__init__(self, inbound_nodes)\n",
    "    \n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Note: this method doesn't has value parameter as we supposed to take\n",
    "        values from inbound_nodes and perform calculation.\n",
    "        \"\"\"\n",
    "        self.value = 0\n",
    "        for inbound_node in self.inbound_nodes:\n",
    "                self.value += inbound_node.value\n",
    "        \n",
    "        return self.value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward Propagation:\n",
    "Like in tensorFlow library, we has to create the computation graph first which gets initialized when we pass values to it and call evaluate. It then checks the computation graph, runs through the computation and give us the output. Similiary, we will implement to menthods in the this library.\n",
    "\n",
    "`topological_sort() and forward_pass()`, In order to define the network we need to define the order of operations on nodes. Given the input to some node depends on the output of others, we need to flatten this computation graph in such a way that all the nodes gets evaluated first whose inputs are needed to calculate the output of current node.\n",
    "\n",
    "To resolve this we will implement kahn's Algorithm which will sort the nodes inthe order of their calculation. The input of `topological_sort()` is `feed_dict: a python dict` and the output is `sorted list of nodes`.\n",
    "\n",
    "Then `forward_pass()` will take this `sorted_nodes` list and do a forward pass on each node and gives back the final `output_node` which will contain the final value of the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topological_sort(feed_dict):\n",
    "    \"\"\"\n",
    "    Sort generic nodes in topological order using Kahn's Algorithm.\n",
    "\n",
    "    `feed_dict`: A dictionary where the key is a `Input` node and the value is the respective value feed to that node.\n",
    "\n",
    "    Returns a list of sorted nodes.\n",
    "    \"\"\"\n",
    "\n",
    "    input_nodes = [n for n in feed_dict.keys()]\n",
    "\n",
    "    G = {}\n",
    "    nodes = [n for n in input_nodes]\n",
    "    while len(nodes) > 0:\n",
    "        n = nodes.pop(0)\n",
    "        if n not in G:\n",
    "            G[n] = {'in': set(), 'out': set()}\n",
    "        for m in n.outbound_nodes:\n",
    "            if m not in G:\n",
    "                G[m] = {'in': set(), 'out': set()}\n",
    "            G[n]['out'].add(m)\n",
    "            G[m]['in'].add(n)\n",
    "            nodes.append(m)\n",
    "\n",
    "    L = []\n",
    "    S = set(input_nodes)\n",
    "    while len(S) > 0:\n",
    "        n = S.pop()\n",
    "\n",
    "        if isinstance(n, Input):\n",
    "            n.value = feed_dict[n]\n",
    "\n",
    "        L.append(n)\n",
    "        for m in n.outbound_nodes:\n",
    "            G[n]['out'].remove(m)\n",
    "            G[m]['in'].remove(n)\n",
    "            # if no other incoming edges add to S\n",
    "            if len(G[m]['in']) == 0:\n",
    "                S.add(m)\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(output_node, sorted_nodes):\n",
    "    \"\"\"\n",
    "    Performs a forward pass through a list of sorted nodes.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        `output_node`: A node in the graph, should be the output node (have no outgoing edges).\n",
    "        `sorted_nodes`: A topologically sorted list of nodes.\n",
    "\n",
    "    Returns the output Node's value\n",
    "    \"\"\"\n",
    "\n",
    "    for n in sorted_nodes:\n",
    "        n.forward()\n",
    "\n",
    "    return output_node.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 + 5 = 23 (according to miniflow)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script builds and runs a graph with miniflow.\n",
    "\n",
    "There is no need to change anything to solve this quiz!\n",
    "\n",
    "However, feel free to play with the network! Can you also\n",
    "build a network that solves the equation below?\n",
    "\n",
    "(x + y) + y\n",
    "\"\"\"\n",
    "\n",
    "x, y, z = Input(), Input(), Input()\n",
    "\n",
    "f = Add([x, y, z])\n",
    "\n",
    "feed_dict = {x: 10, y: 5, z:8}\n",
    "\n",
    "sorted_nodes = topological_sort(feed_dict)\n",
    "output = forward_pass(f, sorted_nodes)\n",
    "\n",
    "# NOTE: because topological_sort sets the values for the `Input` nodes we could also access\n",
    "# the value for x with x.value (same goes for y).\n",
    "print(\"{} + {} = {} (according to miniflow)\".format(feed_dict[x], feed_dict[y], output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratualtions!, on building your first feed forward nueral network.\n",
    "Next this will be to compare output value:`y'` with true value:`y`, calculate error term and do backprop to adjust the weights to improve the model.\n",
    "\n",
    "\n",
    "## Learning and loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, So far we implemented forward pass of neural network which outputs y'. The real learning starts after this in NN, the error term is calculated and backpropagation happen where the network weights and bias are adjusted using chain rule. This weights and bias are updated by a simple formula: W - alpha * del_of_errorterm_wrt_W, where alpha is learning rate.\n",
    "\n",
    "To understand the learning happening in the network, let's implement `Linear` node which takes weight, bias and input(x) and outputs `$\\sum(W_i*X_i) + bias$`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Node):\n",
    "    def __init__(self, inputs, weights, bias):\n",
    "        Node.__init__(self, [inputs, weights, bias])\n",
    "\n",
    "        # NOTE: The weights and bias properties here are not\n",
    "        # numbers, but rather references to other nodes.\n",
    "        # The weight and bias values are stored within the\n",
    "        # respective nodes.\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set self.value to the value of the linear function output.\n",
    "        \n",
    "        \"\"\"\n",
    "        inputs = self.inbound_nodes[0].value\n",
    "        weights = self.inbound_nodes[1].value\n",
    "        bias = self.inbound_nodes[2].value\n",
    "        self.value = bias\n",
    "        \n",
    "        print(\"inbound_nodes[0].value: \"+ str(inputs))\n",
    "        print(\"inbound_nodes[1].value: \"+ str(weights))\n",
    "        print(\"inbound_nodes[2].value: \"+ str(bias))\n",
    "        for x, w in zip(inputs, weights):\n",
    "            self.value += x * w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inbound_nodes[0].value: [6, 14, 3]\n",
      "inbound_nodes[1].value: [0.5, 0.25, 1.4]\n",
      "inbound_nodes[2].value: 2\n",
      "12.7\n"
     ]
    }
   ],
   "source": [
    "inputs, weights, bias = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(inputs, weights, bias)\n",
    "\n",
    "feed_dict = {\n",
    "    inputs: [6, 14, 3],\n",
    "    weights: [0.5, 0.25, 1.4],\n",
    "    bias: 2\n",
    "}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "print(output) # should be 12.7 with this example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Modify Linear#forward so that it linearly transforms\n",
    "input matrices, weights matrices and a bias vector to\n",
    "an output.\n",
    "\"\"\"\n",
    "class Linear(Node):\n",
    "    def __init__(self, X, W, b):\n",
    "        # Notice the ordering of the input nodes passed to the\n",
    "        # Node constructor.\n",
    "        Node.__init__(self, [X, W, b])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the linear transform output.\n",
    "\n",
    "        Your code goes here!\n",
    "        \"\"\"\n",
    "        inputs = self.inbound_nodes[0].value\n",
    "        weights = self.inbound_nodes[1].value\n",
    "        bias = self.inbound_nodes[2].value\n",
    "        \n",
    "#         print(\"inbound_nodes[0].value: \"+ str(inputs))\n",
    "#         print(\"inbound_nodes[1].value: \"+ str(weights))\n",
    "#         print(\"inbound_nodes[2].value: \"+ str(bias))\n",
    "        self.value = np.dot(inputs, weights) + bias\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-9.  4.]\n",
      " [-9.  4.]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "The setup is similar to the prevous `Linear` node you wrote\n",
    "except you're now using NumPy arrays instead of python lists.\n",
    "\n",
    "Update the Linear class in miniflow.py to work with\n",
    "numpy vectors (arrays) and matrices.\n",
    "\n",
    "Test your code here!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(f, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[-9., 4.],\n",
    "[-9., 4.]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function: Sigmoid function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sigmoid(Node):\n",
    "    \"\"\"\n",
    "    You need to fix the `_sigmoid` and `forward` methods.\n",
    "    \"\"\"\n",
    "    def __init__(self, node):\n",
    "        Node.__init__(self, [node])\n",
    "\n",
    "    def _sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        This method is separate from `forward` because it\n",
    "        will be used later with `backward` as well.\n",
    "\n",
    "        `x`: A numpy array-like object.\n",
    "\n",
    "        Return the result of the sigmoid function.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        return 1./(1 + np.exp(-x))\n",
    "\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Set the value of this node to the result of the\n",
    "        sigmoid function, `_sigmoid`.\n",
    "\n",
    "        Your code here!\n",
    "        \"\"\"\n",
    "        # This is a dummy value to prevent numpy errors\n",
    "        # if you test without changing this method.\n",
    "        inputs = self.inbound_nodes[0].value\n",
    "                \n",
    "        self.value = self._sigmoid(inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.23394576e-04 9.82013790e-01]\n",
      " [1.23394576e-04 9.82013790e-01]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This network feeds the output of a linear transform\n",
    "to the sigmoid function.\n",
    "\n",
    "Finish implementing the Sigmoid class in miniflow.py!\n",
    "\n",
    "Feel free to play around with this network, too!\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "X, W, b = Input(), Input(), Input()\n",
    "\n",
    "f = Linear(X, W, b)\n",
    "g = Sigmoid(f)\n",
    "\n",
    "X_ = np.array([[-1., -2.], [-1, -2]])\n",
    "W_ = np.array([[2., -3], [2., -3]])\n",
    "b_ = np.array([-3., -5])\n",
    "\n",
    "feed_dict = {X: X_, W: W_, b: b_}\n",
    "\n",
    "graph = topological_sort(feed_dict)\n",
    "output = forward_pass(g, graph)\n",
    "\n",
    "\"\"\"\n",
    "Output should be:\n",
    "[[  1.23394576e-04   9.82013790e-01]\n",
    " [  1.23394576e-04   9.82013790e-01]]\n",
    "\"\"\"\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error term or Cost Function\n",
    "### MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSE(Node):\n",
    "    def __init__(self, y, a):\n",
    "        \"\"\"\n",
    "        The mean squared error cost function.\n",
    "        Should be used as the last node for a network.\n",
    "        \"\"\"\n",
    "        # Call the base class' constructor.\n",
    "        Node.__init__(self, [y, a])\n",
    "\n",
    "    def forward(self):\n",
    "        \"\"\"\n",
    "        Calculates the mean squared error.\n",
    "        \"\"\"\n",
    "        # NOTE: We reshape these to avoid possible matrix/vector broadcast\n",
    "        # errors.\n",
    "        #\n",
    "        # For example, if we subtract an array of shape (3,) from an array of shape\n",
    "        # (3,1) we get an array of shape(3,3) as the result when we want\n",
    "        # an array of shape (3,1) instead.\n",
    "        #\n",
    "        # Making both arrays (3,1) insures the result is (3,1) and does\n",
    "        # an elementwise subtraction as expected.\n",
    "        y = self.inbound_nodes[0].value.reshape(-1, 1)\n",
    "        a = self.inbound_nodes[1].value.reshape(-1, 1)\n",
    "        # TODO: your code here\n",
    "        #self.value = np.divide(np.sum(np.square(np.subtract(y, a))),y.shape[0])\n",
    "        self.value = np.mean(np.square(np.subtract(y, a)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.416666666666668\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y, a = Input(), Input()\n",
    "cost = MSE(y, a)\n",
    "\n",
    "y_ = np.array([1, 2, 3])\n",
    "a_ = np.array([4.5, 5, 10])\n",
    "\n",
    "feed_dict = {y: y_, a: a_}\n",
    "graph = topological_sort(feed_dict)\n",
    "# forward pass\n",
    "forward_pass(cost, graph)\n",
    "\n",
    "\"\"\"\n",
    "Expected output\n",
    "\n",
    "23.4166666667\n",
    "\"\"\"\n",
    "print(cost.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation : way to calculate amount of change in error cost due to wach weight and bias\n",
    "### Gradient Descent : way to adjust weights and bias\n",
    "\n",
    "Great! We've successfully calculated a full forward pass and found the cost. Next we need to start a backwards pass, which starts with backpropagation. Backpropagation is the process by which the network runs error values backwards.\n",
    "\n",
    "During this process, the network calculates the way in which the weights need to change (also called the gradient) to reduce the overall error of the network. Changing the weights usually occurs through a technique called gradient descent.\n",
    "\n",
    "Making sense of the purpose of backpropagation comes more easily after you work through the intended outcome. I'll come back to backpropagation in a bit, but first, I want to dive deeper into gradient descent.\n",
    "\n",
    "![alt text](images/SGD.png \"Point in 3-d surface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a point on a surface in three dimensional space. In real-life, a ball sitting on the slope of a valley makes a nice analogy. In this case, the height of the point represents the difference between the current output of the network and the correct output given the current parameter values (hence why you need data with known outputs). Each dimension of the plane represents another parameter to the network. A network with m parameters would be a hypersurface of m dimensions.\n",
    "\n",
    "(Imagining more than three dimensions is tricky. The good news is that the ball and valley example describes the behavior of gradient descent well, the only difference between three dimensional and n dimensional situations being the number of parameters in the calculations.)\n",
    "\n",
    "In the ideal situation, the ball rests at the bottom of the valley, indicating the minimum difference between the output of the network and the known correct output.\n",
    "\n",
    "The learning process starts with random weights and biases. In the ball analogy, the ball starts at a random point near the valley.\n",
    "\n",
    "Gradient descent works by first calculating the slope of the plane at the current point, which includes calculating the partial derivatives of the loss with respect to all of the weights. This set of partial derivatives is called the gradient. Then it uses the gradient to modify the weights such that the next forward pass through the network moves the output lower in the hypersurface. Physically, this would be the same as measuring the slope of the valley at the location of the ball, and then moving the ball a small amount in the direction of the slope. Over time, it's possible to find the bottom of the valley with many small movements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_update(x, gradx, learning_rate):\n",
    "    \"\"\"\n",
    "    Performs a gradient descent update.\n",
    "    \"\"\"\n",
    "    # TODO: Implement gradient descent.\n",
    "    \n",
    "    # Return the new value for x\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 1: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 2: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 3: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 4: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 5: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 6: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 7: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 8: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 9: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 10: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 11: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 12: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 13: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 14: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 15: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 16: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 17: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 18: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 19: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 20: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 21: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 22: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 23: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 24: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 25: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 26: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 27: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 28: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 29: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 30: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 31: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 32: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 33: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 34: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 35: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 36: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 37: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 38: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 39: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 40: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 41: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 42: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 43: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 44: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 45: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 46: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 47: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 48: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 49: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 50: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 51: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 52: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 53: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 54: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 55: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 56: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 57: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 58: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 59: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 60: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 61: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 62: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 63: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 64: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 65: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 66: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 67: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 68: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 69: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 70: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 71: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 72: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 73: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 74: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 75: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 76: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 77: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 78: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 79: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 80: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 81: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 82: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 83: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 84: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 85: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 86: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 87: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 88: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 89: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 90: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 91: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 92: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 93: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 94: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 95: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 96: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 97: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 98: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 99: Cost = 29909966.000, x = 10938.000\n",
      "EPOCH 100: Cost = 29909966.000, x = 10938.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Given the starting point of any `x` gradient descent\n",
    "should be able to find the minimum value of x for the\n",
    "cost function `f` defined below.\n",
    "\"\"\"\n",
    "import random\n",
    "\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    Quadratic function.\n",
    "\n",
    "    It's easy to see the minimum value of the function\n",
    "    is 5 when is x=0.\n",
    "    \"\"\"\n",
    "    return x**2 + 5\n",
    "\n",
    "\n",
    "def df(x):\n",
    "    \"\"\"\n",
    "    Derivative of `f` with respect to `x`.\n",
    "    \"\"\"\n",
    "    return 2*x\n",
    "\n",
    "\n",
    "# Random number between 0 and 10,000. Feel free to set x whatever you like.\n",
    "x = random.randint(0, 10000)\n",
    "# TODO: Set the learning rate\n",
    "learning_rate = 0.001\n",
    "epochs = 100\n",
    "\n",
    "for i in range(epochs+1):\n",
    "    cost = f(x)\n",
    "    gradx = df(x)\n",
    "    print(\"EPOCH {}: Cost = {:.3f}, x = {:.3f}\".format(i, cost, gradx))\n",
    "    x = gradient_descent_update(x, gradx, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdc",
   "language": "python",
   "name": "sdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
