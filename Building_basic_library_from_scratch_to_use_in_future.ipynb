{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 2:\n",
    "*Developing basic building blocks to use it as library into our machine learning project.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sofmax function:\n",
    "*softmax is activation fuction in nueral network. Softmax is similar to sigmoid fuction, it gives the probability of class to which given input belongs to.*\n",
    "\n",
    "*softmax gives the probabilty always greater or equal to zero, thus converting every negative number to positive value.*\n",
    "\n",
    "formula is simple:\n",
    "\n",
    "f(x) = $(e^x_i)/\\sum(e^x_i)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that takes as input a list of numbers, and returns\n",
    "# the list of values given by the softmax function.\n",
    "# def softmax(L):\n",
    "#     prob_of_class = []\n",
    "#     exp_of_L = np.exp(L)\n",
    "#     sum_exp = np.sum(np.exp(L))\n",
    "#     for i in exp_of_L:\n",
    "#         softmax = i / sum_exp\n",
    "#         prob_of_class.append(softmax)\n",
    "#     return prob_of_class\n",
    "\n",
    "def softmax(L):\n",
    "    expL = np.exp(L)\n",
    "    return np.divide (expL, expL.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "[[0.02676393 0.05941935 0.31010893 0.39869423 0.20501355]]\n"
     ]
    }
   ],
   "source": [
    "L = np.random.randn(1,5)\n",
    "print(type(L))\n",
    "soft = softmax(L)\n",
    "print(str(soft))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood:\n",
    "suppose, there are 2 models\n",
    "given our score, model\n",
    "1. which gives 80% probability to get admission to a university.\n",
    "2. which gives 55% probability to get admission to a university.\n",
    "\n",
    "if we get admitted then we say model 1 is more accurate and if we didn't get admission then model 2 is more accurate as it gives low probability to get admitted.\n",
    "\n",
    "so, Maximum likelihood is the maximum probability given to events that happened to us, whether it is acceptance or rejection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy\n",
    "So we're getting somewhere, there's definitely a connection between probabilities and error functions, and it's called Cross-Entropy. This concept is tremendously popular in many fields, including Machine Learning. Let's dive more into the formula, and actually code it!\n",
    "\n",
    "\n",
    "Cross entropy measures the error from the probability. if entropy is more means the given events are less likely to happen and vice versa.\n",
    "\n",
    "so p(x1) * p(x2) * p(x3) gives the total chances of this event to occur but as multiplication is expensive and when no. of events increases this leads to very small value and small change in any one input gives drastic change in output.\n",
    "hence, we take natural log.\n",
    "since we are taking logs of probabilities which has to be between  (0,1) and log of values between 0 and 1 is negative number hence we are multiplying the output by -1 to adjust this.\n",
    "\n",
    "0.6*0.2*0.1*0.7=0.0084\n",
    "ln(0.6*0.2*0.1*0.7) = ln(0.6)+ln(0.2)+ln(0.1)+ln(0.7)\n",
    "= -ln(0.6)-ln(0.2)-ln(0.1)-ln(0.7) = 4.8 which means we have big error to reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function that takes as input two lists Y, P,\n",
    "# and returns the float corresponding to their cross-entropy.\n",
    "def cross_entropy(Y, P):\n",
    "    Y = np.float(Y)\n",
    "    P = np.float(P)\n",
    "    return -np.sum(Y*np.log(P) + (1-Y)* np.log(1-P))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network output:\n",
      "None\n",
      "Amount of Error:\n",
      "None\n",
      "Change in Weights:\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "learnrate = 0.5\n",
    "x = np.array([1, 2])\n",
    "y = np.array(0.5)\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([0.5, -0.5])\n",
    "\n",
    "# Calculate one gradient descent step for each weight\n",
    "# TODO: Calculate output of neural network\n",
    "nn_output = None\n",
    "\n",
    "# TODO: Calculate error of neural network\n",
    "error = None\n",
    "\n",
    "# TODO: Calculate change in weights\n",
    "del_w = None\n",
    "\n",
    "print('Neural Network output:')\n",
    "print(nn_output)\n",
    "print('Amount of Error:')\n",
    "print(error)\n",
    "print('Change in Weights:')\n",
    "print(del_w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdc",
   "language": "python",
   "name": "sdc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
